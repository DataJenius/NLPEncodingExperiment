{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPEncodingExperiment_get_word2vec_embeddings.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataJenius/NLPEncodingExperiment/blob/main/python/NLPEncodingExperiment_get_word2vec_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4h2vEnMa7BK",
        "outputId": "2cdf87db-90d7-47e3-9328-ae92171a3462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 22.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 45.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 66.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.3 frozenlist-1.3.0 fsspec-2022.2.0 huggingface-hub-0.4.0 multidict-6.0.2 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.21.5)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "###########################################\n",
        "# install all dependencies\n",
        "!pip install datasets\n",
        "!pip install gensim==3.8.3\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################################################\n",
        "# define our model using StepByStep framework by Daniel Voigt Godoy\n",
        "# https://pytorchstepbystep.com/\n",
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)    \n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "from config import *\n",
        "config_chapter11()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSFvCVKp4tov",
        "outputId": "0c52a1a4-2680-42c1-9b92-55c7e5188c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from GitHub repo to Colab...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################################\n",
        "# load all dependencies\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Split\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora, downloader\n",
        "from gensim.parsing.preprocessing import *\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import downloader\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from stepbystep.v4 import StepByStep\n",
        "\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "sKWjQlQva9fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# embeddings available via Gensim\n",
        "# https://github.com/RaRe-Technologies/gensim-data\n",
        "\n",
        "# this is huge: vocab of 3,000,000 tokens, 1,662.8MB -- takes about 7 minutes\n",
        "word2vec = downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "QDabPbvIib10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20045e62-3de4-46d7-a040-3ce5290d2bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# play with embeddings as an example\n",
        "word2vec[\"king\"]+word2vec[\"woman\"]-word2vec[\"man\"]\n",
        "word2vec.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-RS6rjLU1ZT",
        "outputId": "859beb14-a171-4593-c315-ad903f76ce18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071),\n",
              " ('monarch', 0.6189674139022827),\n",
              " ('princess', 0.5902431011199951),\n",
              " ('crown_prince', 0.5499460697174072),\n",
              " ('prince', 0.5377321243286133),\n",
              " ('kings', 0.5236844420433044),\n",
              " ('Queen_Consort', 0.5235945582389832),\n",
              " ('queens', 0.518113374710083),\n",
              " ('sultan', 0.5098593235015869),\n",
              " ('monarchy', 0.5087411999702454)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# Generate our embedding vocabulary and save as local TXT file \n",
        "# using code from https://leanpub.com/pytorch\n",
        "def make_vocab_from_wv(wv, folder=None, special_tokens=None):\n",
        "    if folder is not None:\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "    words = wv.index2word\n",
        "    if special_tokens is not None:\n",
        "        to_add = []\n",
        "        for special_token in special_tokens:\n",
        "            if special_token not in words:\n",
        "                to_add.append(special_token)\n",
        "        words = to_add + words\n",
        "                \n",
        "    with open(os.path.join(folder, 'vocab.txt'), 'w') as f:\n",
        "        for word in words:\n",
        "            f.write(f'{word}\\n') \n",
        "\n",
        "# run the function and make vocab on training data\n",
        "make_vocab_from_wv(word2vec, \n",
        "                   'word2vec_vocab/', \n",
        "                   special_tokens=['[PAD]', '[UNK]'])            "
      ],
      "metadata": {
        "id": "gsX9DNabdZBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# add special embeddings for missing tokens [PAD], [UNK]\n",
        "# just a vector of 0's equal to the number of embedding dimensions (300 for word2vec)\n",
        "special_embeddings = np.zeros((2, word2vec.vector_size))\n",
        "extended_embeddings = np.concatenate([special_embeddings, word2vec.vectors], axis=0)\n",
        "\n",
        "# hold special_embeddings in a tensor with embeddings \n",
        "extended_embeddings = torch.as_tensor(extended_embeddings).float()\n",
        "torch_embeddings = nn.Embedding.from_pretrained(extended_embeddings)"
      ],
      "metadata": {
        "id": "ieVvdf7tyoCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################\n",
        "# use the BERT tokenizer with our selected embedding vocab\n",
        "word2vec_tokenizer = BertTokenizer('word2vec_vocab/vocab.txt')"
      ],
      "metadata": {
        "id": "yqJTG51YqoFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "# show how the tokenization works\n",
        "new_sentences = [\"I love Luke Skywalker, but I hate Gandalf the Grey.\",                 \n",
        "                 \"I love Gandalf the Grey, but I hate Luke Skywalker.\"] \n",
        "\n",
        "# encode using the tokenizer\n",
        "new_ids = word2vec_tokenizer(new_sentences, \n",
        "                              truncation=True,\n",
        "                              padding=True, \n",
        "                              max_length=250, \n",
        "                              add_special_tokens=False, \n",
        "                              return_tensors='pt')['input_ids']\n",
        "\n",
        "# evaluate the tokenization\n",
        "print(new_sentences[0])\n",
        "print(new_ids[0].squeeze().tolist())\n",
        "print(word2vec_tokenizer.convert_ids_to_tokens(new_ids[0].squeeze().tolist()))\n",
        "print(\"\\n\")\n",
        "print(new_sentences[1])\n",
        "print(new_ids[1].squeeze().tolist())\n",
        "print(word2vec_tokenizer.convert_ids_to_tokens(new_ids[1].squeeze().tolist()))\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4qnosMszbcM",
        "outputId": "ce6bfb2e-7b57-4478-ba65-9ca4e2ad66b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love Luke Skywalker, but I hate Gandalf the Grey.\n",
            "[4503, 748, 442387, 1553862, 1, 35, 4503, 4295, 881832, 246699, 266393, 13, 653992, 331340, 1]\n",
            "['i', 'love', 'luke', 'skywalker', '[UNK]', 'but', 'i', 'hate', 'ganda', '##l', '##f', 'the', 'gre', '##y', '[UNK]']\n",
            "\n",
            "\n",
            "I love Gandalf the Grey, but I hate Luke Skywalker.\n",
            "[4503, 748, 881832, 246699, 266393, 13, 653992, 331340, 1, 35, 4503, 4295, 442387, 1553862, 1]\n",
            "['i', 'love', 'ganda', '##l', '##f', 'the', 'gre', '##y', '[UNK]', 'but', 'i', 'hate', 'luke', 'skywalker', '[UNK]']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################\n",
        "# our labelled, raw comment data is on github\n",
        "all_files = ['https://raw.githubusercontent.com/DataJenius/NLPEncodingExperiment/main/data/comments/selected/selected_reddit_comments_group1.csv',\n",
        "             'https://raw.githubusercontent.com/DataJenius/NLPEncodingExperiment/main/data/comments/selected/selected_reddit_comments_group2.csv',\n",
        "             'https://raw.githubusercontent.com/DataJenius/NLPEncodingExperiment/main/data/comments/selected/selected_reddit_comments_group3.csv',\n",
        "             'https://raw.githubusercontent.com/DataJenius/NLPEncodingExperiment/main/data/comments/selected/selected_reddit_comments_group4.csv',\n",
        "             'https://raw.githubusercontent.com/DataJenius/NLPEncodingExperiment/main/data/comments/selected/selected_reddit_comments_group5.csv']\n",
        "\n",
        "# concat into a single dataframe and shuffle the contents\n",
        "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
        "df_all_data   = pd.concat(df_from_each_file, ignore_index=True)\n",
        "df_all_data = df_all_data.sample(frac=1).reset_index(drop=True)\n",
        "print(df_all_data.shape)\n",
        "print(df_all_data.head())"
      ],
      "metadata": {
        "id": "zzc-Wu5tPTfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23279f4-7426-4eec-8a59-a36fc42768bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 8)\n",
            "   msg_id  token_count  my_group my_role  label       source  \\\n",
            "0   16859           13         5    test      0  /r/StarWars   \n",
            "1   11549           13         1   train      0  /r/StarWars   \n",
            "2   44109           16         5    test      1      /r/lotr   \n",
            "3   10211           40         2   train      0  /r/StarWars   \n",
            "4   34200           16         5    test      1      /r/lotr   \n",
            "\n",
            "                                            raw_text  \\\n",
            "0  Yup, almost all guns in Star Wars are based on...   \n",
            "1  No, that would be the Clone Wars animated movi...   \n",
            "2  I mean that with the little we know thus far t...   \n",
            "3  Seriously Robert Rodriguez is by far the worst...   \n",
            "4  Frodo and Sam parting, and the Army of the Dea...   \n",
            "\n",
            "                                          clean_text  \n",
            "0  yup almost all guns in star wars are based on ...  \n",
            "1  no that would be the clone wars animated movie...  \n",
            "2  i mean that with the little we know thus far t...  \n",
            "3  seriously robert rodriguez is by far the worst...  \n",
            "4  frodo and sam parting and the army of the dead...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################################################################\n",
        "# get our word2vec sentence embeddings for all comments \n",
        "all_ids = word2vec_tokenizer(df_all_data['raw_text'].tolist(), \n",
        "                               truncation=True,\n",
        "                               padding=True, \n",
        "                               max_length=500, \n",
        "                               add_special_tokens=False, \n",
        "                               return_tensors='pt')['input_ids']\n",
        "\n",
        "# load all of our token embeddings for eac token_ids for each msg\n",
        "token_embeddings = torch_embeddings(all_ids)\n",
        "token_embeddings.shape\n",
        "\n",
        "# get sentence embedding via boe_mean (Bag O' Embeddings)\n",
        "boe_mean = nn.EmbeddingBag.from_pretrained(extended_embeddings, mode='mean')\n",
        "sentence_vectors = boe_mean(all_ids)\n",
        "sentence_vectors.shape\n",
        "\n",
        "# format these 300 dimensional embeddings into a df with the msg_id\n",
        "df_sentence_embeddings = pd.DataFrame(sentence_vectors.numpy())\n",
        "df_sentence_embeddings[\"msg_id\"]=df_all_data['msg_id']\n",
        "print(df_sentence_embeddings.head())\n",
        "print(df_sentence_embeddings.shape)\n",
        "\n",
        "# save embeddings to local CSV\n",
        "file_name = 'all_word2vec_embeddings.csv'\n",
        "df_sentence_embeddings.to_csv(file_name, index=False) \n",
        "files.download(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "_y0DpdWF4r0o",
        "outputId": "06cb7081-e251-45ea-c327-72f14e5a453e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6  \\\n",
            "0  0.003339  0.003475  0.004821  0.008059 -0.003259 -0.000954  0.001748   \n",
            "1  0.005939  0.004278 -0.000164  0.006519 -0.000781 -0.000419  0.000534   \n",
            "2  0.000456  0.004279  0.000278  0.009820 -0.013631  0.001497 -0.000738   \n",
            "3  0.002580  0.008441  0.001129  0.022415 -0.010199 -0.004328  0.000669   \n",
            "4  0.000863  0.006457  0.001117  0.006546  0.001303 -0.007947 -0.004253   \n",
            "\n",
            "          7         8         9  ...       291       292       293       294  \\\n",
            "0 -0.006635  0.003307  0.008280  ...  0.000618 -0.003854  0.004181 -0.002837   \n",
            "1 -0.006690  0.007150  0.005826  ... -0.000449 -0.005705  0.002481 -0.000282   \n",
            "2 -0.008488  0.008086  0.011036  ...  0.001485 -0.007454  0.005443 -0.000002   \n",
            "3 -0.020939  0.018410  0.016906  ...  0.007107 -0.016543  0.016347 -0.007113   \n",
            "4 -0.013011  0.006457  0.014033  ... -0.003262 -0.011293  0.006958 -0.003605   \n",
            "\n",
            "        295       296       297       298       299  msg_id  \n",
            "0 -0.003555  0.002419 -0.002766  0.001846  0.001345   16859  \n",
            "1 -0.002208 -0.005845 -0.003975 -0.002733  0.000536   11549  \n",
            "2 -0.002394 -0.002563 -0.004884  0.010341 -0.003710   44109  \n",
            "3 -0.011698 -0.009108 -0.017133  0.007988  0.000031   10211  \n",
            "4 -0.003492 -0.006515 -0.005765 -0.004627  0.003529   34200  \n",
            "\n",
            "[5 rows x 301 columns]\n",
            "(10000, 301)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f9c35a75-7a39-461f-8c76-f6d4b2703516\", \"all_word2vec_embeddings.csv\", 39136583)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}